{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb2iUS-emKyC",
        "outputId": "9950d17d-a3e2-4a96-d69a-bce107ae7420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 37.6MB/s]\n",
            "/tmp/ipython-input-1016166834.py:62: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  imgs.append(Image.fromarray(arr, mode=\"RGB\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gaussian] üretim → single:1800, combo:200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1016166834.py:72: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  return Image.fromarray(arr, mode=\"RGB\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gaussian] tamamlandı: 2000 görsel kaydedildi.\n",
            "[poisson] üretim → single:1800, combo:200\n",
            "[poisson] tamamlandı: 2000 görsel kaydedildi.\n",
            "[salt] üretim → single:1800, combo:200\n",
            "[salt] tamamlandı: 2000 görsel kaydedildi.\n",
            "[speckle] üretim → single:1800, combo:200\n",
            "[speckle] tamamlandı: 2000 görsel kaydedildi.\n",
            "[perlin] üretim → single:1800, combo:200\n",
            "[perlin] tamamlandı: 2000 görsel kaydedildi.\n",
            "[OK] Toplam 10000 görsel -> noisy_dataset\n"
          ]
        }
      ],
      "source": [
        "# create_noisy_dataset_combo.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "5 gürültü sınıfı (gaussian, poisson, salt, speckle, perlin) için:\n",
        "- Sınıf başına 2000 görüntü üretir (varsayılan)\n",
        "- 1800 tek gürültü, 200 kombinasyon (dominant gürültü = klasör)\n",
        "- Gürültü maskeleme: tam sayfa, yarım (dikey/yatay), şerit/çizgi\n",
        "- Kombinasyonda maskeler çakışmaz, dominantın alanı daha büyük ayarlanır\n",
        "\n",
        "Örnek:\n",
        "python create_noisy_dataset_combo.py --out_dir ./noisy_dataset \\\n",
        "  --per_class 2000 --single_per_class 1800 --combo_per_class 200 --resize 128 --seed 42\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ----------------------------\n",
        "# Yardımcılar\n",
        "# ----------------------------\n",
        "def ensure_dir(p: Path):\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_source_images_from_dir(source_dir: Path) -> List[Image.Image]:\n",
        "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\"}\n",
        "    files = [p for p in source_dir.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"Kaynak görsel bulunamadı: {source_dir}\")\n",
        "    imgs = []\n",
        "    for p in files:\n",
        "        try:\n",
        "            imgs.append(Image.open(p).convert(\"RGB\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not imgs:\n",
        "        raise RuntimeError(\"Geçerli görüntü açılamadı.\")\n",
        "    return imgs\n",
        "\n",
        "def download_cifar10(root: Path, resize: int = 128) -> List[Image.Image]:\n",
        "    ensure_dir(root)\n",
        "    if resize and resize > 0:\n",
        "        tfm = transforms.Compose([\n",
        "            transforms.Resize((resize, resize), interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "    else:\n",
        "        tfm = transforms.ToTensor()\n",
        "    ds = datasets.CIFAR10(root=str(root), train=True, download=True, transform=tfm)\n",
        "    imgs: List[Image.Image] = []\n",
        "    for i in range(len(ds)):\n",
        "        t, _ = ds[i]  # C,H,W tensor [0,1]\n",
        "        arr = (t.numpy().transpose(1,2,0) * 255.0).clip(0,255).astype(np.uint8)\n",
        "        imgs.append(Image.fromarray(arr, mode=\"RGB\"))\n",
        "    return imgs  # ~50k\n",
        "\n",
        "def to_np(img: Image.Image) -> np.ndarray:\n",
        "    if img.mode != \"RGB\":\n",
        "        img = img.convert(\"RGB\")\n",
        "    return np.asarray(img, dtype=np.float32) / 255.0\n",
        "\n",
        "def to_img(arr: np.ndarray) -> Image.Image:\n",
        "    arr = np.clip(arr * 255.0, 0, 255).astype(np.uint8)\n",
        "    return Image.fromarray(arr, mode=\"RGB\")\n",
        "\n",
        "def rand_float(a: float, b: float) -> float:\n",
        "    return a + (b - a) * random.random()\n",
        "\n",
        "# ----------------------------\n",
        "# Gürültüler\n",
        "# ----------------------------\n",
        "def add_gaussian(img: Image.Image) -> Image.Image:\n",
        "    x = to_np(img)\n",
        "    sigma = rand_float(0.01, 0.08)\n",
        "    n = np.random.normal(0.0, sigma, x.shape).astype(np.float32)\n",
        "    y = np.clip(x + n, 0, 1)\n",
        "    return to_img(y)\n",
        "\n",
        "def add_poisson(img: Image.Image) -> Image.Image:\n",
        "    x = to_np(img)\n",
        "    vals = 2 ** np.ceil(rand_float(3, 6))\n",
        "    y = np.random.poisson(x * vals) / vals\n",
        "    y = np.clip(y, 0, 1).astype(np.float32)\n",
        "    return to_img(y)\n",
        "\n",
        "def add_salt(img: Image.Image) -> Image.Image:\n",
        "    x = to_np(img)\n",
        "    prob = rand_float(0.003, 0.02)\n",
        "    rnd = np.random.rand(*x.shape[:2])\n",
        "    salt = rnd < (prob / 2.0)\n",
        "    pepper = rnd > (1 - prob / 2.0)\n",
        "    y = x.copy()\n",
        "    y[salt] = 1.0\n",
        "    y[pepper] = 0.0\n",
        "    return to_img(y)\n",
        "\n",
        "def add_speckle(img: Image.Image) -> Image.Image:\n",
        "    x = to_np(img)\n",
        "    sigma = rand_float(0.02, 0.15)\n",
        "    n = np.random.normal(0.0, sigma, x.shape).astype(np.float32)\n",
        "    y = np.clip(x + x * n, 0, 1)\n",
        "    return to_img(y)\n",
        "\n",
        "# Basit Perlin benzeri aydınlatma\n",
        "def _fade(t: np.ndarray) -> np.ndarray:\n",
        "    return 6*t**5 - 15*t**4 + 10*t**3\n",
        "\n",
        "def _lerp(a: np.ndarray, b: np.ndarray, t: np.ndarray) -> np.ndarray:\n",
        "    return a + t * (b - a)\n",
        "\n",
        "def _perlin_2d(h, w, scale):\n",
        "    gx = int(math.ceil(w / scale)) + 2\n",
        "    gy = int(math.ceil(h / scale)) + 2\n",
        "    theta = np.random.rand(gy, gx) * 2 * np.pi\n",
        "    grads = np.dstack((np.cos(theta), np.sin(theta)))\n",
        "    yy, xx = np.mgrid[0:h, 0:w]\n",
        "    x = xx / scale; y = yy / scale\n",
        "    x0 = x.astype(int); x1 = x0 + 1\n",
        "    y0 = y.astype(int); y1 = y0 + 1\n",
        "\n",
        "    def grad(ix, iy): return grads[iy, ix]\n",
        "    def dot_grid(ix, iy, x, y):\n",
        "        g = grad(ix, iy); dx = x - ix; dy = y - iy\n",
        "        return dx * g[...,0] + dy * g[...,1]\n",
        "\n",
        "    n00 = dot_grid(x0, y0, x, y)\n",
        "    n10 = dot_grid(x1, y0, x, y)\n",
        "    n01 = dot_grid(x0, y1, x, y)\n",
        "    n11 = dot_grid(x1, y1, x, y)\n",
        "    u = _fade(x - x0); v = _fade(y - y0)\n",
        "    nx0 = _lerp(n00, n10, u)\n",
        "    nx1 = _lerp(n01, n11, u)\n",
        "    nxy = _lerp(nx0, nx1, v)\n",
        "    nmin, nmax = nxy.min(), nxy.max()\n",
        "    if nmax - nmin < 1e-6: return np.zeros_like(nxy)\n",
        "    return (nxy - nmin) / (nmax - nmin)\n",
        "\n",
        "def add_perlin(img: Image.Image) -> Image.Image:\n",
        "    x = to_np(img)\n",
        "    h, w = x.shape[:2]\n",
        "    octaves = random.choice([1,2,3])\n",
        "    scale = rand_float(48, 128)\n",
        "    strength = rand_float(0.1, 0.35)\n",
        "    noise = np.zeros((h, w), dtype=np.float32)\n",
        "    amp = 1.0; total = 0.0\n",
        "    for o in range(octaves):\n",
        "        noise += _perlin_2d(h, w, scale/(2**o)) * amp\n",
        "        total += amp\n",
        "        amp *= 0.5\n",
        "    noise /= max(total, 1e-6)\n",
        "    noise = (noise - 0.5) * 2.0\n",
        "    mask = 1.0 + strength * noise[..., None]\n",
        "    y = np.clip(x * mask, 0, 1)\n",
        "    return to_img(y)\n",
        "\n",
        "NOISE_FUNCS = {\n",
        "    \"gaussian\": add_gaussian,\n",
        "    \"poisson\": add_poisson,\n",
        "    \"salt\": add_salt,\n",
        "    \"speckle\": add_speckle,\n",
        "    \"perlin\": add_perlin,\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Maske üretimi (overlap yok)\n",
        "# ----------------------------\n",
        "def mask_full(h: int, w: int) -> np.ndarray:\n",
        "    return np.ones((h, w), dtype=bool)\n",
        "\n",
        "def mask_half(h: int, w: int) -> np.ndarray:\n",
        "    if random.random() < 0.5:  # dikey yarım\n",
        "        x0 = 0 if random.random() < 0.5 else w//2\n",
        "        x1 = w//2 if x0 == 0 else w\n",
        "        m = np.zeros((h, w), dtype=bool)\n",
        "        m[:, x0:x1] = True\n",
        "    else:  # yatay yarım\n",
        "        y0 = 0 if random.random() < 0.5 else h//2\n",
        "        y1 = h//2 if y0 == 0 else h\n",
        "        m = np.zeros((h, w), dtype=bool)\n",
        "        m[y0:y1, :] = True\n",
        "    return m\n",
        "\n",
        "def mask_strip(h: int, w: int) -> np.ndarray:\n",
        "    # çizgi/şerit: yatay ya da dikey, genişlik rastgele\n",
        "    m = np.zeros((h, w), dtype=bool)\n",
        "    if random.random() < 0.5:\n",
        "        # dikey şerit\n",
        "        width = max(2, int(rand_float(0.05, 0.2) * w))\n",
        "        x0 = random.randint(0, w - width)\n",
        "        m[:, x0:x0+width] = True\n",
        "    else:\n",
        "        # yatay şerit\n",
        "        height = max(2, int(rand_float(0.05, 0.2) * h))\n",
        "        y0 = random.randint(0, h - height)\n",
        "        m[y0:y0+height, :] = True\n",
        "    return m\n",
        "\n",
        "MASK_FUNCS = [mask_full, mask_half, mask_strip]\n",
        "\n",
        "def make_mask(h: int, w: int, target_area_ratio: float, max_trials: int = 50) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    İstenen alan oranına en yakın maskeyi basitçe dener. (full/half/strip)\n",
        "    \"\"\"\n",
        "    best = None; best_diff = 1e9\n",
        "    for _ in range(max_trials):\n",
        "        m = random.choice(MASK_FUNCS)(h, w)\n",
        "        ar = m.mean()\n",
        "        diff = abs(ar - target_area_ratio)\n",
        "        if diff < best_diff:\n",
        "            best, best_diff = m, diff\n",
        "            if best_diff < 0.02:  # yeterince yakın\n",
        "                break\n",
        "    return best\n",
        "\n",
        "def apply_noise_with_mask(img: Image.Image, mask: np.ndarray, noise_fn) -> Image.Image:\n",
        "    base = to_np(img)\n",
        "    noisy_img = noise_fn(img)\n",
        "    noisy = to_np(noisy_img)\n",
        "    out = base.copy()\n",
        "    out[mask] = noisy[mask]\n",
        "    return to_img(out)\n",
        "\n",
        "# ----------------------------\n",
        "# Üretim mantığı\n",
        "# ----------------------------\n",
        "def generate_single(img: Image.Image, noise_name: str) -> Tuple[Image.Image, Dict]:\n",
        "    h, w = img.size[1], img.size[0]\n",
        "    m = random.choice(MASK_FUNCS)(h, w)\n",
        "    out = apply_noise_with_mask(img, m, NOISE_FUNCS[noise_name])\n",
        "    meta = {\"type\": \"single\", \"primary\": noise_name, \"mask_area\": float(m.mean())}\n",
        "    return out, meta\n",
        "\n",
        "def generate_combo(img: Image.Image, primary: str, partner: str) -> Tuple[Image.Image, Dict]:\n",
        "    \"\"\"\n",
        "    İki gürültü uygula; maskeler çakışmasın. primary alanı partner'dan büyük olsun.\n",
        "    \"\"\"\n",
        "    x = to_np(img); h, w = x.shape[:2]\n",
        "    # alan oranları: primary %40–80, partner %10–40, toplam <= 90% (biraz boşluk kalsın)\n",
        "    primary_ar = rand_float(0.45, 0.7)\n",
        "    partner_ar = rand_float(0.1, min(0.35, 0.9 - primary_ar - 0.05))\n",
        "    # maske üret\n",
        "    m1 = make_mask(h, w, primary_ar)\n",
        "    # partner maskesi m1 ile overlap etmeyecek şekilde dene\n",
        "    trials = 0\n",
        "    while True:\n",
        "        m2 = make_mask(h, w, partner_ar)\n",
        "        if not (m1 & m2).any():\n",
        "            break\n",
        "        trials += 1\n",
        "        if trials > 200:\n",
        "            # vazgeç → m2'yi m1 dışına kırp\n",
        "            m2 = np.logical_and(m2, ~m1)\n",
        "            break\n",
        "\n",
        "    # uygula\n",
        "    tmp = apply_noise_with_mask(img, m1, NOISE_FUNCS[primary])\n",
        "    out = apply_noise_with_mask(tmp, m2, NOISE_FUNCS[partner])\n",
        "\n",
        "    meta = {\n",
        "        \"type\": \"combo\",\n",
        "        \"primary\": primary,\n",
        "        \"partner\": partner,\n",
        "        \"primary_area\": float(m1.mean()),\n",
        "        \"partner_area\": float(m2.mean()),\n",
        "        \"non_overlap\": bool(not (m1 & m2).any())\n",
        "    }\n",
        "    return out, meta\n",
        "\n",
        "def build_pool(source_dir: Optional[str], cache_dir: Path, resize: int) -> List[Image.Image]:\n",
        "    if source_dir:\n",
        "        imgs = load_source_images_from_dir(Path(source_dir))\n",
        "        if resize and resize > 0:\n",
        "            imgs = [img.resize((resize, resize), Image.BICUBIC) for img in imgs]\n",
        "        return imgs\n",
        "    return download_cifar10(cache_dir, resize=resize)\n",
        "\n",
        "def generate_dataset(\n",
        "    out_dir: Path,\n",
        "    per_class: int = 2000,\n",
        "    single_per_class: int = 1800,\n",
        "    combo_per_class: int = 200,\n",
        "    resize: int = 128,\n",
        "    seed: int = 42,\n",
        "    source_dir: Optional[str] = None\n",
        "):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    classes = list(NOISE_FUNCS.keys())\n",
        "    for c in classes:\n",
        "        ensure_dir(out_dir / c)\n",
        "\n",
        "    pool = build_pool(source_dir, Path(\"./_cache\"), resize)\n",
        "    n_pool = len(pool)\n",
        "    if n_pool == 0:\n",
        "        raise RuntimeError(\"Kaynak havuz boş.\")\n",
        "\n",
        "    # her sınıf için üretim\n",
        "    for primary in classes:\n",
        "        print(f\"[{primary}] üretim → single:{single_per_class}, combo:{combo_per_class}\")\n",
        "        # tek gürültü\n",
        "        for idx in range(single_per_class):\n",
        "            src = pool[(idx + seed) % n_pool]\n",
        "            out_img, meta = generate_single(src, primary)\n",
        "            fname = f\"{primary}_single_{idx:05d}_area{meta['mask_area']:.2f}.png\"\n",
        "            out_img.save(out_dir / primary / fname)\n",
        "\n",
        "        # kombinasyon: primary + partner (dominant primary)\n",
        "        partners = [c for c in classes if c != primary]\n",
        "        for idx in range(combo_per_class):\n",
        "            src = pool[(idx * 7 + seed) % n_pool]\n",
        "            partner = random.choice(partners)\n",
        "            out_img, meta = generate_combo(src, primary, partner)\n",
        "            fname = (\n",
        "                f\"{primary}_combo_{partner}_{idx:05d}\"\n",
        "                f\"_p{meta['primary_area']:.2f}_s{meta['partner_area']:.2f}.png\"\n",
        "            )\n",
        "            out_img.save(out_dir / primary / fname)\n",
        "\n",
        "        print(f\"[{primary}] tamamlandı: {per_class} görsel kaydedildi.\")\n",
        "\n",
        "    total = len(classes) * per_class\n",
        "    print(f\"[OK] Toplam {total} görsel -> {out_dir}\")\n",
        "\n",
        "# ----------------------------\n",
        "# CLI\n",
        "# ----------------------------\n",
        "def parse_args():\n",
        "    ap = argparse.ArgumentParser(description=\"Noisy dataset (single + dominant-combo, mask-based, no overlap)\")\n",
        "    ap.add_argument(\"--out_dir\", type=str, default=\"./noisy_dataset\", help=\"Çıktı klasörü\")\n",
        "    ap.add_argument(\"--per_class\", type=int, default=2000, help=\"Sınıf başına toplam adet\")\n",
        "    ap.add_argument(\"--single_per_class\", type=int, default=1800, help=\"Sınıf başına tek gürültü adedi\")\n",
        "    ap.add_argument(\"--combo_per_class\", type=int, default=200, help=\"Sınıf başına kombinasyon adedi\")\n",
        "    ap.add_argument(\"--resize\", type=int, default=128, help=\"Hedef çözünürlük (px). 0=kapalı\")\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    ap.add_argument(\"--source_dir\", type=str, default=None, help=\"Kendi temiz görsellerin (varsa)\")\n",
        "    return ap.parse_args()\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    if args.single_per_class + args.combo_per_class != args.per_class:\n",
        "        raise ValueError(\"single_per_class + combo_per_class = per_class olmalı.\")\n",
        "    out_dir = Path(args.out_dir); ensure_dir(out_dir)\n",
        "    generate_dataset(\n",
        "        out_dir=out_dir,\n",
        "        per_class=args.per_class,\n",
        "        single_per_class=args.single_per_class,\n",
        "        combo_per_class=args.combo_per_class,\n",
        "        resize=args.resize,\n",
        "        seed=args.seed,\n",
        "        source_dir=args.source_dir\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        # Colab için manuel argüman ver\n",
        "        args = argparse.Namespace(\n",
        "            out_dir=\"./noisy_dataset\",\n",
        "            per_class=2000,\n",
        "            single_per_class=1800,\n",
        "            combo_per_class=200,\n",
        "            resize=128,\n",
        "            seed=42,\n",
        "            source_dir=None\n",
        "        )\n",
        "        out_dir = Path(args.out_dir)\n",
        "        ensure_dir(out_dir)\n",
        "        generate_dataset(\n",
        "            out_dir=out_dir,\n",
        "            per_class=args.per_class,\n",
        "            single_per_class=args.single_per_class,\n",
        "            combo_per_class=args.combo_per_class,\n",
        "            resize=args.resize,\n",
        "            seed=args.seed,\n",
        "            source_dir=args.source_dir\n",
        "        )\n",
        "    else:\n",
        "        main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Tüm random seed'leri sabitler\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "def create_split_directories(output_path):\n",
        "    \"\"\"Train/Test/Val klasörlerini oluşturur\"\"\"\n",
        "    splits = ['train', 'test', 'val']\n",
        "    classes = ['gaussian', 'perlin', 'poisson', 'salt', 'speckle']\n",
        "\n",
        "    for split in splits:\n",
        "        for class_name in classes:\n",
        "            dir_path = Path(output_path) / split / class_name\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"Klasör yapısı oluşturuldu: {output_path}\")\n",
        "\n",
        "\n",
        "def get_files_per_class(dataset_path):\n",
        "    \"\"\"Her sınıf için dosya listelerini alır\"\"\"\n",
        "    classes = ['gaussian', 'perlin', 'poisson', 'salt', 'speckle']\n",
        "    class_files = {}\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_path = Path(dataset_path) / class_name\n",
        "        if class_path.exists():\n",
        "            files = [f for f in class_path.glob('*') if\n",
        "                     f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']]\n",
        "            class_files[class_name] = files\n",
        "            print(f\"{class_name}: {len(files)} dosya\")\n",
        "        else:\n",
        "            print(f\"Uyarı: {class_path} klasörü bulunamadı!\")\n",
        "            class_files[class_name] = []\n",
        "\n",
        "    return class_files\n",
        "\n",
        "\n",
        "def split_and_copy_files(class_files, output_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Dosyaları train/val/test olarak böler ve kopyalar\n",
        "    train_ratio + val_ratio + test_ratio = 1.0 olmalı\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    split_info = {\n",
        "        'train': {},\n",
        "        'val': {},\n",
        "        'test': {}\n",
        "    }\n",
        "\n",
        "    total_files = 0\n",
        "\n",
        "    for class_name, files in class_files.items():\n",
        "        if len(files) == 0:\n",
        "            print(f\"Uyarı: {class_name} sınıfında dosya yok!\")\n",
        "            continue\n",
        "\n",
        "        total_files += len(files)\n",
        "\n",
        "        # Önce train ve geçici (val+test) ayırımı\n",
        "        train_files, temp_files = train_test_split(\n",
        "            files,\n",
        "            train_size=train_ratio,\n",
        "            random_state=seed,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Geçici dosyaları val ve test olarak ayır\n",
        "        # val_ratio ve test_ratio'yu geçici dosyalar üzerinden hesapla\n",
        "        val_size = val_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "        val_files, test_files = train_test_split(\n",
        "            temp_files,\n",
        "            train_size=val_size,\n",
        "            random_state=seed,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        # Dosya sayılarını kaydet\n",
        "        split_info['train'][class_name] = len(train_files)\n",
        "        split_info['val'][class_name] = len(val_files)\n",
        "        split_info['test'][class_name] = len(test_files)\n",
        "\n",
        "        # Dosyaları kopyala\n",
        "        splits_data = {\n",
        "            'train': train_files,\n",
        "            'val': val_files,\n",
        "            'test': test_files\n",
        "        }\n",
        "\n",
        "        for split_name, file_list in splits_data.items():\n",
        "            dest_dir = Path(output_path) / split_name / class_name\n",
        "\n",
        "            for i, file_path in enumerate(file_list):\n",
        "                # Hedef dosya adını oluştur (çakışmaları önlemek için)\n",
        "                dest_file = dest_dir / f\"{class_name}_{i:04d}{file_path.suffix}\"\n",
        "                shutil.copy2(file_path, dest_file)\n",
        "\n",
        "        print(f\"{class_name}: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
        "\n",
        "    # Özet bilgileri yazdır\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"DATASET BÖLÜNME ÖZETİ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for split_name in ['train', 'val', 'test']:\n",
        "        total_split = sum(split_info[split_name].values())\n",
        "        print(f\"\\n{split_name.upper()}:\")\n",
        "        for class_name in split_info[split_name]:\n",
        "            count = split_info[split_name][class_name]\n",
        "            percentage = (count / total_files) * 100 if total_files > 0 else 0\n",
        "            print(f\"  {class_name}: {count} dosya ({percentage:.1f}%)\")\n",
        "        print(f\"  TOPLAM: {total_split} dosya\")\n",
        "\n",
        "    print(f\"\\nTOPLAM DOSYA SAYISI: {total_files}\")\n",
        "    print(f\"Seed kullanıldı: {seed}\")\n",
        "\n",
        "    return split_info\n",
        "\n",
        "\n",
        "def save_split_info(split_info, output_path, seed):\n",
        "    \"\"\"Bölünme bilgilerini txt dosyasına kaydeder\"\"\"\n",
        "    info_file = Path(output_path) / \"dataset_split_info.txt\"\n",
        "\n",
        "    with open(info_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"DATASET BÖLÜNME BİLGİLERİ\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\")\n",
        "        f.write(f\"Kullanılan Seed: {seed}\\n\")\n",
        "        f.write(f\"Oluşturulma Tarihi: {Path(__file__).stat().st_mtime}\\n\\n\")\n",
        "\n",
        "        total_files = sum(sum(class_dict.values()) for class_dict in split_info.values())\n",
        "\n",
        "        for split_name in ['train', 'val', 'test']:\n",
        "            total_split = sum(split_info[split_name].values())\n",
        "            f.write(f\"{split_name.upper()}:\\n\")\n",
        "            for class_name in ['gaussian', 'perlin', 'poisson', 'salt', 'speckle']:\n",
        "                count = split_info[split_name].get(class_name, 0)\n",
        "                percentage = (count / total_files) * 100 if total_files > 0 else 0\n",
        "                f.write(f\"  {class_name}: {count} dosya ({percentage:.1f}%)\\n\")\n",
        "            f.write(f\"  TOPLAM: {total_split} dosya\\n\\n\")\n",
        "\n",
        "        f.write(f\"GENEL TOPLAM: {total_files} dosya\\n\")\n",
        "\n",
        "    print(f\"Bölünme bilgileri kaydedildi: {info_file}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Konfigürasyon\n",
        "    SEED = 42\n",
        "    INPUT_DATASET_PATH = \"noisy_dataset\"  # Ana dataset klasörünüz\n",
        "    OUTPUT_DATASET_PATH = \"split_dataset\"  # Bölünmüş dataset kayıt yeri\n",
        "\n",
        "    # Bölünme oranları (toplamı 1.0 olmalı)\n",
        "    TRAIN_RATIO = 0.7  # %70 eğitim\n",
        "    VAL_RATIO = 0.15  # %15 doğrulama\n",
        "    TEST_RATIO = 0.15  # %15 test\n",
        "\n",
        "    print(\"Gürültü Sınıflandırması Dataset Bölme Script'i\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Kaynak Dataset: {INPUT_DATASET_PATH}\")\n",
        "    print(f\"Hedef Dataset: {OUTPUT_DATASET_PATH}\")\n",
        "    print(f\"Train: {TRAIN_RATIO * 100}%, Val: {VAL_RATIO * 100}%, Test: {TEST_RATIO * 100}%\")\n",
        "    print(f\"Seed: {SEED}\")\n",
        "    print()\n",
        "\n",
        "    # Seed'i sabitler\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # Kaynak dataset kontrolü\n",
        "    if not Path(INPUT_DATASET_PATH).exists():\n",
        "        print(f\"Hata: {INPUT_DATASET_PATH} klasörü bulunamadı!\")\n",
        "        return\n",
        "\n",
        "    # Hedef klasör yapısını oluştur\n",
        "    create_split_directories(OUTPUT_DATASET_PATH)\n",
        "\n",
        "    # Her sınıf için dosyaları al\n",
        "    class_files = get_files_per_class(INPUT_DATASET_PATH)\n",
        "\n",
        "    # Dosyaları böl ve kopyala\n",
        "    split_info = split_and_copy_files(\n",
        "        class_files,\n",
        "        OUTPUT_DATASET_PATH,\n",
        "        train_ratio=TRAIN_RATIO,\n",
        "        val_ratio=VAL_RATIO,\n",
        "        test_ratio=TEST_RATIO,\n",
        "        seed=SEED\n",
        "    )\n",
        "\n",
        "    # Bölünme bilgilerini kaydet\n",
        "    save_split_info(split_info, OUTPUT_DATASET_PATH, SEED)\n",
        "\n",
        "    print(\"\\nDataset bölme işlemi tamamlandı!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pRZo2oLymo77",
        "outputId": "589fd0fc-7221-4b30-c454-85e71e52666d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gürültü Sınıflandırması Dataset Bölme Script'i\n",
            "==================================================\n",
            "Kaynak Dataset: noisy_dataset\n",
            "Hedef Dataset: split_dataset\n",
            "Train: 70.0%, Val: 15.0%, Test: 15.0%\n",
            "Seed: 42\n",
            "\n",
            "Klasör yapısı oluşturuldu: split_dataset\n",
            "gaussian: 2000 dosya\n",
            "perlin: 2000 dosya\n",
            "poisson: 2000 dosya\n",
            "salt: 2000 dosya\n",
            "speckle: 2000 dosya\n",
            "gaussian: Train=1400, Val=300, Test=300\n",
            "perlin: Train=1400, Val=300, Test=300\n",
            "poisson: Train=1400, Val=300, Test=300\n",
            "salt: Train=1400, Val=300, Test=300\n",
            "speckle: Train=1400, Val=300, Test=300\n",
            "\n",
            "==================================================\n",
            "DATASET BÖLÜNME ÖZETİ\n",
            "==================================================\n",
            "\n",
            "TRAIN:\n",
            "  gaussian: 1400 dosya (14.0%)\n",
            "  perlin: 1400 dosya (14.0%)\n",
            "  poisson: 1400 dosya (14.0%)\n",
            "  salt: 1400 dosya (14.0%)\n",
            "  speckle: 1400 dosya (14.0%)\n",
            "  TOPLAM: 7000 dosya\n",
            "\n",
            "VAL:\n",
            "  gaussian: 300 dosya (3.0%)\n",
            "  perlin: 300 dosya (3.0%)\n",
            "  poisson: 300 dosya (3.0%)\n",
            "  salt: 300 dosya (3.0%)\n",
            "  speckle: 300 dosya (3.0%)\n",
            "  TOPLAM: 1500 dosya\n",
            "\n",
            "TEST:\n",
            "  gaussian: 300 dosya (3.0%)\n",
            "  perlin: 300 dosya (3.0%)\n",
            "  poisson: 300 dosya (3.0%)\n",
            "  salt: 300 dosya (3.0%)\n",
            "  speckle: 300 dosya (3.0%)\n",
            "  TOPLAM: 1500 dosya\n",
            "\n",
            "TOPLAM DOSYA SAYISI: 10000\n",
            "Seed kullanıldı: 42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3594095234.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3594095234.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# Bölünme bilgilerini kaydet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0msave_split_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDataset bölme işlemi tamamlandı!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3594095234.py\u001b[0m in \u001b[0;36msave_split_info\u001b[0;34m(split_info, output_path, seed)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Kullanılan Seed: {seed}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Oluşturulma Tarihi: {Path(__file__).stat().st_mtime}\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mtotal_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclass_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # 128 x 1 x 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n",
        "            nn.Linear(32, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.gap(x).flatten(1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# ResNet34 (pretrained & fine-tuned)\n",
        "class ResNet34Pretrained(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ResNet34 (scratch)\n",
        "class ResNet34Scratch(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.resnet34(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ResNet50 (pretrained)\n",
        "class ResNet50Pretrained(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# ResNet50 (scratch)\n",
        "class ResNet50Scratch(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.resnet50(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# EfficientNetB0\n",
        "class EfficientNetB0Custom(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "        in_features = m.classifier[1].in_features\n",
        "        m.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# MobileNetV2\n",
        "class MobileNetV2Custom(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        m = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
        "        m.classifier[1] = nn.Linear(m.classifier[1].in_features, num_classes)\n",
        "        self.model = m\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Deeper CNN with more layers\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # 512 x 1 x 1\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 1024), nn.BatchNorm1d(1024), nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512), nn.BatchNorm1d(512), nn.ReLU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.ReLU(),\n",
        "            nn.Linear(64, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.gap(x).flatten(1)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "ptQAkeptmp_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "from datetime import datetime\n",
        "from models import DeepCNN, ResNet34Pretrained, ResNet34Scratch, EfficientNetB0Custom, MobileNetV2Custom, DeeperCNN, ResNet50Pretrained\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Tüm random seed'leri sabitler\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"GPU/CPU device seçimi\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\" GPU kullanılıyor: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB\")\n",
        "        print(f\"PyTorch CUDA Support: {torch.backends.cudnn.enabled}\")\n",
        "\n",
        "        # GPU'ya test tensörü gönder\n",
        "        test_tensor = torch.randn(1, 1).to(device)\n",
        "        print(f\"Test tensor device: {test_tensor.device}\")\n",
        "\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\" CUDA kullanılamıyor, CPU kullanılıyor\")\n",
        "        print(\"Nedenleri:\")\n",
        "        print(\"- CUDA yüklü değil\")\n",
        "        print(\"- PyTorch CUDA versiyonu yüklü değil\")\n",
        "        print(\"- GPU driver sorunu\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def get_data_transforms():\n",
        "    \"\"\"Veri dönüşüm pipeline'ları\"\"\"\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform\n",
        "\n",
        "\n",
        "def create_data_loaders(dataset_path, batch_size=32, num_workers=4):\n",
        "    \"\"\"Data loader'ları oluşturur\"\"\"\n",
        "    train_transform, val_test_transform = get_data_transforms()\n",
        "\n",
        "    # Datasetleri yükle\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        root=Path(dataset_path) / 'train',\n",
        "        transform=train_transform\n",
        "    )\n",
        "\n",
        "    val_dataset = datasets.ImageFolder(\n",
        "        root=Path(dataset_path) / 'val',\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(\n",
        "        root=Path(dataset_path) / 'test',\n",
        "        transform=val_test_transform\n",
        "    )\n",
        "\n",
        "    # Data loader'ları oluştur\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Sınıf isimleri\n",
        "    class_names = train_dataset.classes\n",
        "    print(f\"Sınıflar: {class_names}\")\n",
        "    print(f\"Train: {len(train_dataset)} örnek\")\n",
        "    print(f\"Val: {len(val_dataset)} örnek\")\n",
        "    print(f\"Test: {len(test_dataset)} örnek\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Bir epoch eğitim\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = output.max(1)\n",
        "        total += target.size(0)\n",
        "        correct += predicted.eq(target).sum().item()\n",
        "\n",
        "        if (batch_idx + 1) % 50 == 0:\n",
        "            print(f'    Batch {batch_idx + 1}/{len(train_loader)}, '\n",
        "                  f'Loss: {running_loss / (batch_idx + 1):.4f}, '\n",
        "                  f'Acc: {100. * correct / total:.2f}%')\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validasyon\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = output.max(1)\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(val_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "    return val_loss, val_acc\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, device, class_names):\n",
        "    \"\"\"Test ve classification report\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
        "            output = model(data)\n",
        "            _, predicted = output.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "            total += target.size(0)\n",
        "            correct += predicted.eq(target).sum().item()\n",
        "\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(\n",
        "        all_targets,\n",
        "        all_preds,\n",
        "        target_names=class_names,\n",
        "        digits=4\n",
        "    )\n",
        "\n",
        "    return test_acc, report, all_preds, all_targets\n",
        "\n",
        "\n",
        "def train_model(model_class, model_name, train_loader, val_loader, test_loader,\n",
        "                class_names, device, epochs=25, lr=0.001):\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"EĞİTİM BAŞLADI: {model_name}\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    model = model_class(num_classes=len(class_names)).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
        "\n",
        "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
        "\n",
        "    # İlk değeri çok düşük başlat ve en az 1 kez set edileceğinden emin ol\n",
        "    best_val_acc = -1.0\n",
        "    best_model_state = None\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss); train_accs.append(train_acc)\n",
        "\n",
        "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "        val_losses.append(val_loss); val_accs.append(val_acc)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_state = model.state_dict()  # kopya almak şart değil, save sırasında serialize edilir\n",
        "            print(f\"  *** Yeni en iyi model! Val Acc: {val_acc:.2f}% ***\")\n",
        "\n",
        "    # Eğer herhangi bir iyileşme olmadıysa (edge case), mevcut ağırlıkları kullan\n",
        "    if best_model_state is None:\n",
        "        best_model_state = model.state_dict()\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nEğitim tamamlandı! Süre: {training_time:.2f} saniye\")\n",
        "    print(f\"En iyi validasyon accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nTest değerlendirmesi yapılıyor...\")\n",
        "    test_acc, report, test_preds, test_targets = test_model(model, test_loader, device, class_names)\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_losses': val_losses,\n",
        "        'val_accs': val_accs,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'test_acc': test_acc,\n",
        "        'test_report': report,\n",
        "        'training_time': training_time,\n",
        "        'test_preds': test_preds,\n",
        "        'test_targets': test_targets,\n",
        "        'best_model_state': best_model_state,   # <-- EKLENDİ\n",
        "    }\n",
        "\n",
        "    return model, results\n",
        "\n",
        "\n",
        "\n",
        "def save_results(results, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    model_name = results['model_name']\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # classification report\n",
        "    report_path = Path(output_dir) / f\"{model_name}_classification_report_{timestamp}.txt\"\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"MODEL: {model_name}\\n\")\n",
        "        f.write(f\"TARIH: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        f.write(f\"En İyi Validasyon Accuracy: {results['best_val_acc']:.4f}%\\n\")\n",
        "        f.write(f\"Test Accuracy: {results['test_acc']:.4f}%\\n\")\n",
        "        f.write(f\"Eğitim Süresi: {results['training_time']:.2f} saniye\\n\")\n",
        "        f.write(\"\\nCLASSIFICATION REPORT:\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        f.write(results['test_report'])\n",
        "\n",
        "    print(f\"Classification report kaydedildi: {report_path}\")\n",
        "\n",
        "    # model ağırlıkları\n",
        "    model_state = results.get('best_model_state', None)\n",
        "    model_path = None\n",
        "    if model_state is not None:\n",
        "        model_path = Path(output_dir) / f\"{model_name}_best_model_{timestamp}.pth\"\n",
        "        torch.save(model_state, model_path)\n",
        "        print(f\"Model kaydedildi: {model_path}\")\n",
        "    else:\n",
        "        print(\"Uyarı: best_model_state bulunamadı, model kaydedilmedi.\")\n",
        "\n",
        "    return report_path, model_path\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Konfigürasyon\n",
        "    SEED = 42\n",
        "    DATASET_PATH = \"split_dataset\"\n",
        "    OUTPUT_DIR = \"training_results\"\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 15\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_WORKERS = 4\n",
        "\n",
        "    # Seed sabitler\n",
        "    set_seed(SEED)\n",
        "\n",
        "    # Device seçimi\n",
        "    device = get_device()\n",
        "\n",
        "    # Veri yükleyicilerini oluştur\n",
        "    print(\"Veri yükleyicileri hazırlanıyor...\")\n",
        "    train_loader, val_loader, test_loader, class_names = create_data_loaders(\n",
        "        DATASET_PATH, BATCH_SIZE, NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Eğitilecek modeller\n",
        "    models_to_train = [\n",
        " \t    (DeepCNN, \"DeepCNN\"),\n",
        "      (ResNet34Pretrained, \"ResNet34_Pretrained\"),\n",
        "      (ResNet34Scratch, \"ResNet34_Scratch\"),\n",
        "      (EfficientNetB0Custom, \"EfficientNetB0\"),\n",
        "      (MobileNetV2Custom, \"MobileNetV2\"),\n",
        "      (DeepCNN, \"DeeperCNN\"),\n",
        "      (ResNet50Pretrained, \"ResNet50_Pretrained\"),\n",
        "      (ResNet50Scratch, \"ResNet50_Scratch\"),\n",
        "    ]\n",
        "\n",
        "    # Tüm sonuçları sakla\n",
        "    all_results = []\n",
        "\n",
        "    print(f\"\\nToplam {len(models_to_train)} model eğitilecek\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
        "\n",
        "    for i, (model_class, model_name) in enumerate(models_to_train):\n",
        "        print(f\"\\n Model {i + 1}/{len(models_to_train)}: {model_name}\")\n",
        "\n",
        "        try:\n",
        "            model, results = train_model(\n",
        "                model_class, model_name,\n",
        "                train_loader, val_loader, test_loader,\n",
        "                class_names, device, EPOCHS, LEARNING_RATE\n",
        "            )\n",
        "\n",
        "            # Sonuçları kaydet (rapor + model dosyası)\n",
        "            report_path, model_path = save_results(results, OUTPUT_DIR)\n",
        "            results['report_path'] = report_path\n",
        "            results['model_path'] = model_path\n",
        "            all_results.append(results)\n",
        "\n",
        "            # Memory temizle\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" {model_name} eğitiminde hata: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Özet rapor\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"TÜM MODELLERİN EĞİTİM ÖZETİ\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    for result in all_results:\n",
        "        print(f\"{result['model_name']:<20} - \"\n",
        "              f\"Val: {result['best_val_acc']:.2f}% | \"\n",
        "              f\"Test: {result['test_acc']:.2f}% | \"\n",
        "              f\"Süre: {result['training_time']:.1f}s\")\n",
        "\n",
        "    # En iyi modeli bul\n",
        "    if all_results:\n",
        "        best_result = max(all_results, key=lambda x: x['test_acc'])\n",
        "        print(f\"\\n EN İYİ MODEL: {best_result['model_name']} \"\n",
        "              f\"(Test Acc: {best_result['test_acc']:.2f}%)\")\n",
        "        print(f\" Kaydedilen dosya: {best_result['model_path']}\")\n",
        "\n",
        "    print(f\"\\nTüm sonuçlar kaydedildi: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "mBJ3gbZcmtAl",
        "outputId": "6291de29-599c-4dae-a216-7552ace581e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPU kullanılıyor: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.7 GB\n",
            "PyTorch CUDA Support: True\n",
            "Test tensor device: cuda:0\n",
            "Veri yükleyicileri hazırlanıyor...\n",
            "Sınıflar: ['gaussian', 'perlin', 'poisson', 'salt', 'speckle']\n",
            "Train: 7000 örnek\n",
            "Val: 1500 örnek\n",
            "Test: 1500 örnek\n",
            "\n",
            "Toplam 2 model eğitilecek\n",
            "Epochs: 15, Batch Size: 32, LR: 0.001\n",
            "\n",
            " Model 1/2: DeeperCNN\n",
            "\n",
            "============================================================\n",
            "EĞİTİM BAŞLADI: DeeperCNN\n",
            "============================================================\n",
            "\n",
            "Epoch 1/15\n",
            "Learning Rate: 0.001000\n",
            "    Batch 50/219, Loss: 1.4924, Acc: 32.31%\n",
            "    Batch 100/219, Loss: 1.3881, Acc: 39.25%\n",
            "    Batch 150/219, Loss: 1.2981, Acc: 44.17%\n",
            "    Batch 200/219, Loss: 1.2141, Acc: 48.61%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2836544433.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2836544433.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             model, results = train_model(\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2836544433.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_class, model_name, train_loader, val_loader, test_loader, class_names, device, epochs, lr)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2836544433.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_models.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# (Opsiyonel) seaborn varsa kullan; yoksa matplotlib ile devam\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    _HAS_SNS = True\n",
        "except Exception:\n",
        "    _HAS_SNS = False\n",
        "\n",
        "# ---- Modeller ----\n",
        "from models import (\n",
        "    DeepCNN,\n",
        "    ResNet34Pretrained,\n",
        "    ResNet34Scratch,\n",
        "    EfficientNetB0Custom,\n",
        "    MobileNetV2Custom,\n",
        "    DeeperCNN,\n",
        "    ResNet50Pretrained\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Yardımcılar\n",
        "# -----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"GPU kullanılıyor: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"CPU kullanılıyor\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def get_test_transform(img_size=224):\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "def create_test_loader(dataset_path, batch_size=32, num_workers=4, img_size=224):\n",
        "    dataset_path = Path(dataset_path)\n",
        "    test_dir = dataset_path / 'test'\n",
        "    if not test_dir.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"'test' klasörü bulunamadı: {test_dir}\\n\"\n",
        "            f\"Yapı şu formatta olmalı:\\n\"\n",
        "            f\"{dataset_path}/test/<class_name>/*.png\"\n",
        "        )\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(\n",
        "        root=test_dir,\n",
        "        transform=get_test_transform(img_size)\n",
        "    )\n",
        "    # Windows/Colab ortamında worker sayısını güvenli tut\n",
        "    if num_workers is None or num_workers < 0:\n",
        "        num_workers = 0\n",
        "\n",
        "    pin_memory = torch.cuda.is_available()\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory\n",
        "    )\n",
        "\n",
        "    class_names = test_dataset.classes\n",
        "    print(f\"Test sınıfları: {class_names}\")\n",
        "    print(f\"Test örnekleri: {len(test_dataset)}\")\n",
        "    return test_loader, class_names\n",
        "\n",
        "\n",
        "def _extract_state_dict(ckpt):\n",
        "    \"\"\"\n",
        "    Farklı kaydetme formatlarını destekler:\n",
        "    - Doğrudan state_dict\n",
        "    - {'state_dict': ...} (Lightning tipik)\n",
        "    - {'model_state_dict': ...} (PyTorch common)\n",
        "    \"\"\"\n",
        "    if isinstance(ckpt, dict):\n",
        "        for key in ['state_dict', 'model_state_dict']:\n",
        "            if key in ckpt and isinstance(ckpt[key], dict):\n",
        "                return ckpt[key]\n",
        "    return ckpt  # zaten state_dict olabilir\n",
        "\n",
        "\n",
        "def load_model(model_class, model_path, num_classes, device):\n",
        "    model = model_class(num_classes=num_classes).to(device)\n",
        "\n",
        "    if model_path is None:\n",
        "        print(\"Checkpoint verilmedi, model rastgele ağırlıklarla yüklendi (sadece değerlendirme).\")\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    try:\n",
        "        ckpt = torch.load(model_path, map_location=device)\n",
        "        state_dict = _extract_state_dict(ckpt)\n",
        "\n",
        "        # Lightning tarzı 'model.' veya 'module.' ön eklerini temizle\n",
        "        cleaned = {}\n",
        "        for k, v in state_dict.items():\n",
        "            if k.startswith('model.'):\n",
        "                cleaned[k[len('model.'):]] = v\n",
        "            elif k.startswith('module.'):\n",
        "                cleaned[k[len('module.'):]] = v\n",
        "            else:\n",
        "                cleaned[k] = v\n",
        "\n",
        "        missing, unexpected = model.load_state_dict(cleaned, strict=False)\n",
        "        if missing:\n",
        "            print(f\"Uyarı: Eksik anahtarlar: {missing}\")\n",
        "        if unexpected:\n",
        "            print(f\"Uyarı: Beklenmeyen anahtarlar: {unexpected}\")\n",
        "\n",
        "        model.eval()\n",
        "        print(f\"Model başarıyla yüklendi: {model_path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Model yüklenirken hata: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_model_detailed(model, test_loader, device, class_names, model_name):\n",
        "    model.eval()\n",
        "    all_preds, all_targets, all_probs = [], [], []\n",
        "    correct, total = 0, 0\n",
        "    class_correct = {i: 0 for i in range(len(class_names))}\n",
        "    class_total = {i: 0 for i in range(len(class_names))}\n",
        "\n",
        "    print(f\"\\n{model_name} test ediliyor...\")\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(test_loader):\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        target = target.to(device, non_blocking=True)\n",
        "\n",
        "        output = model(data)\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        predicted = torch.argmax(output, dim=1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(target.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        for i in range(target.size(0)):\n",
        "            label = target[i].item()\n",
        "            class_total[label] += 1\n",
        "            if predicted[i] == label:\n",
        "                class_correct[label] += 1\n",
        "\n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(test_loader)} işlendi\")\n",
        "\n",
        "    overall_acc = 100.0 * correct / max(total, 1)\n",
        "\n",
        "    class_accuracies = {}\n",
        "    for i, cname in enumerate(class_names):\n",
        "        denom = max(class_total[i], 1)\n",
        "        class_accuracies[cname] = 100.0 * class_correct[i] / denom\n",
        "\n",
        "    report = classification_report(\n",
        "        all_targets, all_preds, target_names=class_names, digits=4, zero_division=0\n",
        "    )\n",
        "    cm = confusion_matrix(all_targets, all_preds, labels=list(range(len(class_names))))\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'overall_accuracy': overall_acc,\n",
        "        'class_accuracies': class_accuracies,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs,\n",
        "        'class_names': class_names\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, model_name, save_path=None):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    if _HAS_SNS:\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "    else:\n",
        "        plt.imshow(cm)\n",
        "        plt.xticks(ticks=range(len(class_names)), labels=class_names, rotation=45, ha='right')\n",
        "        plt.yticks(ticks=range(len(class_names)), labels=class_names)\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, str(cm[i, j]),\n",
        "                         ha='center', va='center')\n",
        "        plt.colorbar()\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"Confusion matrix kaydedildi: {save_path}\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_detailed_results(results, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model_name = results['model_name']\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    report_path = Path(output_dir) / f\"{model_name}_detailed_test_report_{timestamp}.txt\"\n",
        "\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"DETAYLI TEST RAPORU\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        f.write(f\"Model: {model_name}\\n\")\n",
        "        f.write(f\"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Test Accuracy: {results['overall_accuracy']:.4f}%\\n\\n\")\n",
        "\n",
        "        f.write(\"SINIF BAZINDA DOĞRULUK ORANLARI:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for cname, acc in results['class_accuracies'].items():\n",
        "            f.write(f\"{cname:<15}: {acc:.2f}%\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"CLASSIFICATION REPORT:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        f.write(results['classification_report'])\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"CONFUSION MATRIX:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        cm = results['confusion_matrix']\n",
        "\n",
        "        f.write(f\"{'':>12}\")\n",
        "        for cname in results['class_names']:\n",
        "            f.write(f\"{cname[:8]:>8}\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        for i, cname in enumerate(results['class_names']):\n",
        "            f.write(f\"{cname[:10]:>10}  \")\n",
        "            for j in range(len(results['class_names'])):\n",
        "                f.write(f\"{cm[i, j]:>8}\")\n",
        "            f.write(\"\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"İSTATİSTİKLER:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        preds = np.array(results['predictions'])\n",
        "        targs = np.array(results['targets'])\n",
        "        f.write(f\"Toplam test örneği: {len(targs)}\\n\")\n",
        "        f.write(f\"Doğru tahmin: {(preds == targs).sum()}\\n\")\n",
        "        f.write(f\"Yanlış tahmin: {(preds != targs).sum()}\\n\")\n",
        "\n",
        "    print(f\"Detaylı test raporu kaydedildi: {report_path}\")\n",
        "\n",
        "    cm_path = Path(output_dir) / f\"{model_name}_confusion_matrix_{timestamp}.png\"\n",
        "    plot_confusion_matrix(results['confusion_matrix'], results['class_names'], model_name, cm_path)\n",
        "    return report_path, cm_path\n",
        "\n",
        "\n",
        "def compare_models(all_results, output_dir):\n",
        "    if not all_results:\n",
        "        return None\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    comparison_path = Path(output_dir) / f\"model_comparison_{timestamp}.txt\"\n",
        "\n",
        "    with open(comparison_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"MODEL KARŞILAŞTIRMA RAPORU\\n\")\n",
        "        f.write(\"=\" * 60 + \"\\n\")\n",
        "        f.write(f\"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Test edilen model sayısı: {len(all_results)}\\n\\n\")\n",
        "\n",
        "        sorted_results = sorted(all_results, key=lambda x: x['overall_accuracy'], reverse=True)\n",
        "\n",
        "        f.write(\"GENEL PERFORMANS SIRALAMASI:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for i, r in enumerate(sorted_results, 1):\n",
        "            f.write(f\"{i}. {r['model_name']:<20}: {r['overall_accuracy']:.2f}%\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        class_names = all_results[0]['class_names']\n",
        "        f.write(\"SINIF BAZINDA EN İYİ PERFORMANS:\\n\")\n",
        "        f.write(\"-\" * 40 + \"\\n\")\n",
        "        for cname in class_names:\n",
        "            best_model, best_acc = \"\", 0.0\n",
        "            for r in all_results:\n",
        "                acc = r['class_accuracies'][cname]\n",
        "                if acc > best_acc:\n",
        "                    best_acc, best_model = acc, r['model_name']\n",
        "            f.write(f\"{cname:<15}: {best_model:<20} ({best_acc:.2f}%)\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"DETAYLI KARŞILAŞTIRMA:\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "        f.write(f\"{'Model':<20}{'Overall':<10}\")\n",
        "        for cname in class_names:\n",
        "            f.write(f\"{cname[:8]:<10}\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "        for r in sorted_results:\n",
        "            f.write(f\"{r['model_name']:<20}{r['overall_accuracy']:.2f}%{'':>4}\")\n",
        "            for cname in class_names:\n",
        "                f.write(f\"{r['class_accuracies'][cname]:.2f}%{'':>4}\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    print(f\"Model karşılaştırma raporu kaydedildi: {comparison_path}\")\n",
        "    return comparison_path\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Argümanlar\n",
        "# -----------------------------\n",
        "def build_parser():\n",
        "    ap = argparse.ArgumentParser(description=\"Modelleri test et ve karşılaştır\")\n",
        "    ap.add_argument(\"--dataset_path\", type=str, default=\"split_dataset\",\n",
        "                    help=\"train/val/test alt klasörleri olan kök dizin. Burada sadece 'test' kullanılır.\")\n",
        "    ap.add_argument(\"--output_dir\", type=str, default=\"test_results\",\n",
        "                    help=\"Rapor ve görseller için çıktı klasörü\")\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=32)\n",
        "    ap.add_argument(\"--workers\", type=int, default=4)\n",
        "    ap.add_argument(\"--img_size\", type=int, default=224)\n",
        "    ap.add_argument(\"--seed\", type=int, default=42)\n",
        "    # Hangi modeller test edilsin\n",
        "    ap.add_argument(\"--models\", type=str, default=\"DeepCNN,ResNet34Pretrained,ResNet34Scratch,EfficientNetB0Custom,MobileNetV2Custom\",\n",
        "                    help=\"Virgülle ayrık model listesi\")\n",
        "    # Checkpoint klasörü (içinde model_adi.pth varsa otomatik alınır)\n",
        "    ap.add_argument(\"--ckpt_dir\", type=str, default=None,\n",
        "                    help=\"Checkpoint klasörü. Örn: ckpts/ (içinde DeepCNN.pth, ResNet34Pretrained.pth vb.)\")\n",
        "    return ap\n",
        "\n",
        "\n",
        "def resolve_model_list(models_arg: str):\n",
        "    name_to_class = {\n",
        "        \"DeepCNN\": DeepCNN,\n",
        "        \"ResNet34Pretrained\": ResNet34Pretrained,\n",
        "        \"ResNet34Scratch\": ResNet34Scratch,\n",
        "        \"EfficientNetB0Custom\": EfficientNetB0Custom,\n",
        "        \"MobileNetV2Custom\": MobileNetV2Custom,\n",
        "    }\n",
        "    names = [x.strip() for x in models_arg.split(\",\") if x.strip()]\n",
        "    resolved = []\n",
        "    for n in names:\n",
        "        if n not in name_to_class:\n",
        "            print(f\"Uyarı: Bilinmeyen model adı atlandı -> {n}\")\n",
        "            continue\n",
        "        resolved.append((n, name_to_class[n]))\n",
        "    return resolved\n",
        "\n",
        "\n",
        "def find_checkpoint(ckpt_dir: Path, model_name: str):\n",
        "    \"\"\"\n",
        "    ckpt_dir / f\"{model_name}.pth\" varsa döndür, yoksa None\n",
        "    \"\"\"\n",
        "    if ckpt_dir is None:\n",
        "        return None\n",
        "    cand = ckpt_dir / f\"{model_name}.pth\"\n",
        "    return str(cand) if cand.exists() else None\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = build_parser()\n",
        "    # Notebook/Colab'ta gelen tanınmayan argümanları yok say\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    device = get_device()\n",
        "\n",
        "    test_loader, class_names = create_test_loader(\n",
        "        args.dataset_path, args.batch_size, args.workers, args.img_size\n",
        "    )\n",
        "\n",
        "    model_specs = resolve_model_list(args.models)\n",
        "    if not model_specs:\n",
        "        raise ValueError(\"Test edilecek en az bir model ismi verilmeli.\")\n",
        "\n",
        "    ckpt_dir = Path(args.ckpt_dir) if args.ckpt_dir else None\n",
        "\n",
        "    all_results = []\n",
        "    print(f\"Test başlıyor - {len(model_specs)} model\")\n",
        "    print(f\"Test örnekleri: {len(test_loader.dataset)}\")\n",
        "    print(f\"Sınıflar: {class_names}\")\n",
        "\n",
        "    for model_name, model_cls in model_specs:\n",
        "        try:\n",
        "            ckpt_path = find_checkpoint(ckpt_dir, model_name) if ckpt_dir else None\n",
        "            if ckpt_path:\n",
        "                model = load_model(model_cls, ckpt_path, len(class_names), device)\n",
        "                if model is None:\n",
        "                    continue\n",
        "            else:\n",
        "                print(f\"\\n⚠ {model_name}: checkpoint bulunamadı → rastgele ağırlıklarla test edilecek.\")\n",
        "                model = model_cls(num_classes=len(class_names)).to(device)\n",
        "                model.eval()\n",
        "\n",
        "            results = test_model_detailed(model, test_loader, device, class_names, model_name)\n",
        "            all_results.append(results)\n",
        "            save_detailed_results(results, args.output_dir)\n",
        "\n",
        "            del model\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"{model_name} - Test Accuracy: {results['overall_accuracy']:.2f}%\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"{model_name} test edilirken hata: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if len(all_results) > 1:\n",
        "        print(\"\\nModel karşılaştırması yapılıyor...\")\n",
        "        compare_models(all_results, args.output_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TEST ÖZETİ\")\n",
        "    print(\"=\" * 60)\n",
        "    if all_results:\n",
        "        best = max(all_results, key=lambda x: x['overall_accuracy'])\n",
        "        print(f\"En iyi model: {best['model_name']} ({best['overall_accuracy']:.2f}%)\")\n",
        "        print(\"\\nTüm modeller:\")\n",
        "        for r in sorted(all_results, key=lambda x: x['overall_accuracy'], reverse=True):\n",
        "            print(f\"  {r['model_name']:<20}: {r['overall_accuracy']:.2f}%\")\n",
        "    else:\n",
        "        print(\"Hiç model test edilemedi!\")\n",
        "\n",
        "    print(f\"\\nTüm sonuçlar kaydedildi: {args.output_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "2ugugV9Yo-db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Transform imports\n",
        "from torchvision import transforms\n",
        "\n",
        "# Model imports\n",
        "from models import DeepCNN, ResNet34Pretrained, ResNet34Scratch, EfficientNetB0Custom, MobileNetV2Custom, ResNet50Pretrained, DeeperCNN\n",
        "\n",
        "class ModelTester:\n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.class_names = ['gaussian', 'poisson', 'salt', 'speckle', 'perlin']\n",
        "        self.models = {}\n",
        "        self.history_file = \"test_history.json\"\n",
        "        self.load_history()\n",
        "\n",
        "        # ✅ Model path listesi (boş bırakabilirsin veya kendi pathlerini yazabilirsin)\n",
        "        self.model_paths = {\n",
        "            \"DeepCNN\": \"C:/Users/.../DeepCNN_best_model.pth\",\n",
        "            \"ResNet34_Pretrained\": \"C:/Users/.../ResNet34_Pretrained_best_model.pth\",\n",
        "            \"ResNet34_Scratch\": \"C:/Users/.../ResNet34_Scratch_best_model.pth\",\n",
        "            \"EfficientNetB0\": \"C:/Users/.../EfficientNetB0_best_model.pth\",\n",
        "            \"MobileNetV2\": r\"C:\\Users\\stj.skartal\\Desktop\\python\\noisy_classification\\training_results\\MobileNetV2_best_model_20250822_145431.pth\"\n",
        "        }\n",
        "\n",
        "        # Image transform pipeline\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        self.load_models()\n",
        "\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Tüm modelleri yükle\"\"\"\n",
        "        model_configs = [\n",
        "            (DeepCNN, \"DeepCNN\"),\n",
        "            (ResNet34Pretrained, \"ResNet34_Pretrained\"),\n",
        "            (ResNet34Scratch, \"ResNet34_Scratch\"),\n",
        "            (EfficientNetB0Custom, \"EfficientNetB0\"),\n",
        "            (MobileNetV2Custom, \"MobileNetV2\")\n",
        "        ]\n",
        "\n",
        "        print(\"Modeller yükleniyor...\")\n",
        "        for model_class, model_name in model_configs:\n",
        "            try:\n",
        "                # ✅ self.model_paths'te kayıtlı path varsa onu kullan\n",
        "                model_path = self.model_paths.get(model_name, None)\n",
        "\n",
        "                if model_path and os.path.exists(model_path):\n",
        "                    model = model_class(num_classes=len(self.class_names)).to(self.device)\n",
        "                    checkpoint = torch.load(model_path, map_location=self.device)\n",
        "                    model.load_state_dict(checkpoint)\n",
        "                    model.eval()\n",
        "                    print(f\"✓ {model_name} kaydedilmiş ağırlıklarla yüklendi ({model_path})\")\n",
        "                else:\n",
        "                    # Eğer kaydedilmiş model yoksa rastgele ağırlıklarla oluştur\n",
        "                    model = model_class(num_classes=len(self.class_names)).to(self.device)\n",
        "                    model.eval()\n",
        "                    print(f\"⚠ {model_name} rastgele ağırlıklarla oluşturuldu (eğitilmemiş)\")\n",
        "\n",
        "                self.models[model_name] = model\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ {model_name} yüklenemedi: {str(e)}\")\n",
        "\n",
        "\n",
        "    def preprocess_image(self, image):\n",
        "        \"\"\"Görüntüyü model için uygun formata çevir\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            # Eğer dosya yolu ise\n",
        "            image = Image.open(image)\n",
        "\n",
        "        # PIL Image'a çevir\n",
        "        if not isinstance(image, Image.Image):\n",
        "            image = Image.fromarray(image)\n",
        "\n",
        "        # RGB'ye çevir\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        # Transform uygula\n",
        "        tensor = self.transform(image).unsqueeze(0)  # Batch dimension ekle\n",
        "        return tensor.to(self.device)\n",
        "\n",
        "    def predict_single_model(self, model, image_tensor):\n",
        "        \"\"\"Tek model ile tahmin yap\"\"\"\n",
        "        with torch.no_grad():\n",
        "            output = model(image_tensor)\n",
        "            probabilities = torch.softmax(output, dim=1)\n",
        "            confidence, predicted = torch.max(probabilities, 1)\n",
        "\n",
        "        return {\n",
        "            'predicted_class': self.class_names[predicted.item()],\n",
        "            'confidence': float(confidence.item() * 100),\n",
        "            'probabilities': {\n",
        "                self.class_names[i]: float(prob * 100)\n",
        "                for i, prob in enumerate(probabilities[0].cpu().numpy())\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def predict_all_models(self, image):\n",
        "        \"\"\"Tüm modellerle tahmin yap\"\"\"\n",
        "        if image is None:\n",
        "            return \"Lütfen bir resim yükleyin!\", \"\", \"\"\n",
        "\n",
        "        try:\n",
        "            # Görüntüyü hazırla\n",
        "            image_tensor = self.preprocess_image(image)\n",
        "\n",
        "            results = {}\n",
        "            predictions_text = \"🤖 **MODEL TAHMİNLERİ:**\\n\\n\"\n",
        "\n",
        "            for model_name, model in self.models.items():\n",
        "                try:\n",
        "                    result = self.predict_single_model(model, image_tensor)\n",
        "                    results[model_name] = result\n",
        "\n",
        "                    predictions_text += f\"**{model_name}:**\\n\"\n",
        "                    predictions_text += f\"  • Tahmin: **{result['predicted_class']}**\\n\"\n",
        "                    predictions_text += f\"  • Güven: **{result['confidence']:.1f}%**\\n\\n\"\n",
        "\n",
        "                except Exception as e:\n",
        "                    predictions_text += f\"**{model_name}:** Hata - {str(e)}\\n\\n\"\n",
        "\n",
        "            # Geçici sonuçları sakla\n",
        "            self.temp_predictions = results\n",
        "\n",
        "            return predictions_text, self.get_accuracy_stats(), \"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Hata: {str(e)}\", \"\", \"\"\n",
        "\n",
        "    def load_history(self):\n",
        "        \"\"\"Geçmişi yükle\"\"\"\n",
        "        if os.path.exists(self.history_file):\n",
        "            try:\n",
        "                with open(self.history_file, 'r', encoding='utf-8') as f:\n",
        "                    self.history = json.load(f)\n",
        "            except:\n",
        "                self.history = []\n",
        "        else:\n",
        "            self.history = []\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"Geçmişi kaydet\"\"\"\n",
        "        # JSON serileştirme için float32'leri float'a çevir\n",
        "        def convert_floats(obj):\n",
        "            if isinstance(obj, dict):\n",
        "                return {k: convert_floats(v) for k, v in obj.items()}\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_floats(v) for v in obj]\n",
        "            elif isinstance(obj, (np.float32, np.float64)):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, (np.int32, np.int64)):\n",
        "                return int(obj)\n",
        "            return obj\n",
        "\n",
        "        cleaned_history = convert_floats(self.history)\n",
        "\n",
        "        with open(self.history_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(cleaned_history, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def submit_ground_truth(self, true_label):\n",
        "        \"\"\"Gerçek etiketi kaydet ve modelleri değerlendir\"\"\"\n",
        "        if not hasattr(self, 'temp_predictions'):\n",
        "            return \"Önce bir resim yükleyip tahmin yaptırın!\", \"\"\n",
        "\n",
        "        if not true_label:\n",
        "            return \"Lütfen gerçek gürültü tipini seçin!\", \"\"\n",
        "\n",
        "        # Geçmişe ekle\n",
        "        record = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'true_label': true_label,\n",
        "            'predictions': self.temp_predictions\n",
        "        }\n",
        "\n",
        "        self.history.append(record)\n",
        "        self.save_history()\n",
        "\n",
        "        # Sonuçları değerlendir\n",
        "        results_text = f\"✅ **DOĞRULAMA SONUÇLARI** (Gerçek: **{true_label}**)\\n\\n\"\n",
        "\n",
        "        for model_name, prediction in self.temp_predictions.items():\n",
        "            predicted = prediction['predicted_class']\n",
        "            confidence = prediction['confidence']\n",
        "\n",
        "            if predicted == true_label:\n",
        "                results_text += f\"✅ **{model_name}**: DOĞRU! ({confidence:.1f}%)\\n\"\n",
        "            else:\n",
        "                results_text += f\"❌ **{model_name}**: YANLIŞ - {predicted} ({confidence:.1f}%)\\n\"\n",
        "\n",
        "        results_text += f\"\\n📊 **Toplam test sayısı**: {len(self.history)}\"\n",
        "\n",
        "        # Temp predictions'ı temizle\n",
        "        delattr(self, 'temp_predictions')\n",
        "\n",
        "        return results_text, self.get_accuracy_stats()\n",
        "\n",
        "    def get_accuracy_stats(self):\n",
        "        \"\"\"Doğruluk istatistiklerini hesapla\"\"\"\n",
        "        if not self.history:\n",
        "            return \"Henüz test verisi yok.\"\n",
        "\n",
        "        # Model bazında istatistikler\n",
        "        model_stats = {}\n",
        "        for model_name in self.models.keys():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            class_stats = {class_name: {'correct': 0, 'total': 0} for class_name in self.class_names}\n",
        "\n",
        "            for record in self.history:\n",
        "                if model_name in record['predictions']:\n",
        "                    total += 1\n",
        "                    true_label = record['true_label']\n",
        "                    predicted = record['predictions'][model_name]['predicted_class']\n",
        "\n",
        "                    class_stats[true_label]['total'] += 1\n",
        "\n",
        "                    if predicted == true_label:\n",
        "                        correct += 1\n",
        "                        class_stats[true_label]['correct'] += 1\n",
        "\n",
        "            if total > 0:\n",
        "                accuracy = (correct / total) * 100\n",
        "                model_stats[model_name] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'correct': correct,\n",
        "                    'total': total,\n",
        "                    'class_stats': class_stats\n",
        "                }\n",
        "\n",
        "        # İstatistikleri formatla\n",
        "        stats_text = f\"📈 **PERFORMANS İSTATİSTİKLERİ** (Son {len(self.history)} test)\\n\\n\"\n",
        "\n",
        "        # Genel başarı oranları\n",
        "        sorted_models = sorted(model_stats.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
        "\n",
        "        stats_text += \"🏆 **GENEL BAŞARI ORANLARI:**\\n\"\n",
        "        for i, (model_name, stats) in enumerate(sorted_models, 1):\n",
        "            stats_text += f\"{i}. **{model_name}**: {stats['accuracy']:.1f}% ({stats['correct']}/{stats['total']})\\n\"\n",
        "\n",
        "        stats_text += \"\\n📋 **SINIF BAZINDA PERFORMANS:**\\n\"\n",
        "\n",
        "        # Sınıf bazında en iyi modeller\n",
        "        for class_name in self.class_names:\n",
        "            best_acc = 0\n",
        "            best_model = \"\"\n",
        "\n",
        "            for model_name, stats in model_stats.items():\n",
        "                class_total = stats['class_stats'][class_name]['total']\n",
        "                if class_total > 0:\n",
        "                    class_acc = (stats['class_stats'][class_name]['correct'] / class_total) * 100\n",
        "                    if class_acc > best_acc:\n",
        "                        best_acc = class_acc\n",
        "                        best_model = model_name\n",
        "\n",
        "            if best_model:\n",
        "                stats_text += f\"• **{class_name}**: {best_model} ({best_acc:.1f}%)\\n\"\n",
        "\n",
        "        return stats_text\n",
        "\n",
        "    def get_detailed_history(self):\n",
        "        \"\"\"Detaylı geçmiş tablosunu oluştur\"\"\"\n",
        "        if not self.history:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        data = []\n",
        "        for i, record in enumerate(self.history, 1):\n",
        "            row = {\n",
        "                'Test #': i,\n",
        "                'Tarih': record['timestamp'][:19],  # Sadece tarih-saat kısmı\n",
        "                'Gerçek': record['true_label']\n",
        "            }\n",
        "\n",
        "            # Her model için tahmin ve doğruluk ekle\n",
        "            for model_name in self.models.keys():\n",
        "                if model_name in record['predictions']:\n",
        "                    predicted = record['predictions'][model_name]['predicted_class']\n",
        "                    confidence = record['predictions'][model_name]['confidence']\n",
        "                    is_correct = \"✅\" if predicted == record['true_label'] else \"❌\"\n",
        "\n",
        "                    row[f'{model_name}_Tahmin'] = predicted\n",
        "                    row[f'{model_name}_Güven'] = f\"{confidence:.1f}%\"\n",
        "                    row[f'{model_name}_Doğru'] = is_correct\n",
        "\n",
        "            data.append(row)\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "# Global model tester instance\n",
        "model_tester = ModelTester()\n",
        "\n",
        "def predict_interface(image):\n",
        "    \"\"\"Gradio arayüzü için tahmin fonksiyonu\"\"\"\n",
        "    return model_tester.predict_all_models(image)\n",
        "\n",
        "def submit_interface(true_label):\n",
        "    \"\"\"Gradio arayüzü için doğrulama fonksiyonu\"\"\"\n",
        "    return model_tester.submit_ground_truth(true_label)\n",
        "\n",
        "def get_stats_interface():\n",
        "    \"\"\"İstatistikleri getir\"\"\"\n",
        "    return model_tester.get_accuracy_stats()\n",
        "\n",
        "def get_history_interface():\n",
        "    \"\"\"Geçmişi getir\"\"\"\n",
        "    return model_tester.get_detailed_history()\n",
        "\n",
        "# Gradio arayüzü\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Gürültü Sınıflandırma Modeli Test Sistemi\", theme=gr.themes.Soft()) as iface:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # 🔬 Gürültü Sınıflandırma Modeli Test Sistemi\n",
        "\n",
        "        Bu arayüz 5 farklı model ile gürültü tiplerini sınıflandırır ve performanslarını karşılaştırır.\n",
        "\n",
        "        **Kullanım:**\n",
        "        1. Bir resim yükleyin\n",
        "        2. Modellerin tahminlerini görün\n",
        "        3. Gerçek gürültü tipini seçin\n",
        "        4. Sonuçları değerlendirin\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Resim yükleme\n",
        "                image_input = gr.Image(\n",
        "                    type=\"pil\",\n",
        "                    label=\"📷 Test Resmi Yükleyin\",\n",
        "                    height=300\n",
        "                )\n",
        "\n",
        "                # Tahmin butonu\n",
        "                predict_btn = gr.Button(\"🔮 Tahmin Yap\", variant=\"primary\")\n",
        "\n",
        "                # Gerçek etiket seçimi\n",
        "                true_label = gr.Dropdown(\n",
        "                    choices=model_tester.class_names,\n",
        "                    label=\"✅ Gerçek Gürültü Tipi\",\n",
        "                    info=\"Resimin gerçek gürültü tipini seçin\"\n",
        "                )\n",
        "\n",
        "                # Doğrulama butonu\n",
        "                submit_btn = gr.Button(\"📝 Doğrulamayı Kaydet\", variant=\"secondary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                # Tahmin sonuçları\n",
        "                predictions_output = gr.Markdown(\n",
        "                    label=\"🤖 Model Tahminleri\",\n",
        "                    value=\"Model tahminleri burada görünecek...\"\n",
        "                )\n",
        "\n",
        "                # Doğrulama sonuçları\n",
        "                validation_output = gr.Markdown(\n",
        "                    label=\"📊 Doğrulama Sonuçları\",\n",
        "                    value=\"Doğrulama sonuçları burada görünecek...\"\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # İstatistikler\n",
        "                stats_output = gr.Markdown(\n",
        "                    label=\"📈 Performans İstatistikleri\",\n",
        "                    value=model_tester.get_accuracy_stats()\n",
        "                )\n",
        "\n",
        "                # İstatistikleri güncelleme butonu\n",
        "                refresh_stats_btn = gr.Button(\"🔄 İstatistikleri Güncelle\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"### 📋 Test Geçmişi\")\n",
        "                history_output = gr.Dataframe(\n",
        "                    value=model_tester.get_detailed_history(),\n",
        "                    label=\"Detaylı Test Geçmişi\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                refresh_history_btn = gr.Button(\"🔄 Geçmişi Güncelle\")\n",
        "\n",
        "        # Event handlers\n",
        "        predict_btn.click(\n",
        "            fn=predict_interface,\n",
        "            inputs=[image_input],\n",
        "            outputs=[predictions_output, stats_output, validation_output]\n",
        "        )\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=submit_interface,\n",
        "            inputs=[true_label],\n",
        "            outputs=[validation_output, stats_output]\n",
        "        )\n",
        "\n",
        "        refresh_stats_btn.click(\n",
        "            fn=get_stats_interface,\n",
        "            outputs=[stats_output]\n",
        "        )\n",
        "\n",
        "        refresh_history_btn.click(\n",
        "            fn=get_history_interface,\n",
        "            outputs=[history_output]\n",
        "        )\n",
        "\n",
        "        # Otomatik güncellemeler\n",
        "        submit_btn.click(\n",
        "            fn=get_history_interface,\n",
        "            outputs=[history_output]\n",
        "        )\n",
        "\n",
        "    return iface\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Gradio arayüzü başlatılıyor...\")\n",
        "    print(f\"📱 Cihaz: {model_tester.device}\")\n",
        "    print(f\"🔢 Yüklenen model sayısı: {len(model_tester.models)}\")\n",
        "\n",
        "    # Arayüzü oluştur ve başlat\n",
        "    interface = create_interface()\n",
        "    interface.launch(\n",
        "        share=False,  # Public link oluştur\n",
        "        server_name=\"0.0.0.0\",  # Tüm IP'lerden erişim\n",
        "        server_port=7860,  # Port\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "id": "HoVre2OImwqd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}